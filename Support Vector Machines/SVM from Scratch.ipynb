{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c85957b",
   "metadata": {},
   "source": [
    "# Implementing Support Vector Machines (SVM) from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a7593",
   "metadata": {},
   "source": [
    "## What are Support Vectors?\n",
    "\n",
    "Support Vectors are the data points that lie closest to the decision boundary (hyperplane).\n",
    "\n",
    "- They are the most critical points in the dataset because:\n",
    "\n",
    "    - The decision boundary is defined only by these points.\n",
    "\n",
    "    - If you remove all other points and keep only support vectors, the SVM solution stays the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1857c32",
   "metadata": {},
   "source": [
    "**SVM Classifier**: `wTx + b = 0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9079e3",
   "metadata": {},
   "source": [
    "### Linear SVM\n",
    "\n",
    "When data is linearly separable, SVM tries to find a straight line (2D) or hyperplane (nD) that separates classes with maximum margin.\n",
    "\n",
    "### Non-linear SVM\n",
    "\n",
    "In real-world data, classes are not linearly separable. Instead of computing the mapping explicitly, SVM uses the Kernel Trick.\n",
    "\n",
    "- A kernel function computes the similarity between two points in a (possibly) higher-dimensional space without explicitly mapping them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c407e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Soft Margin vs Hard Margin\n",
    "\n",
    "- **Hard Margin SVM**: No misclassifications allowed (only works with perfectly separable data).\n",
    "\n",
    "- **Soft Margin SVM**: Allows some misclassification using slack variables ùúâùëñ\n",
    "\n",
    "- Controlled by C (regularization parameter):\n",
    "\n",
    "    - High C ‚Üí less tolerance for misclassification (narrow margin).\n",
    "\n",
    "    - Low C ‚Üí more tolerance (wider margin)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6b256",
   "metadata": {},
   "source": [
    "### For optimization we use Gradient Descent\n",
    "\n",
    "Gradient Descent is an optimization algorithm used for minimizing the loss function in various machine learning algorithms. It is used for updating the parameters of the learning model.\n",
    "\n",
    "- `w = w - Œ±*dw`\n",
    "\n",
    "- `b = b - Œ±*db`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88600ade",
   "metadata": {},
   "source": [
    "**Our goal is to reduce the loss function and find the best weights and bias values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33df73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22345014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportVectorClassifier:\n",
    "    \"\"\"\n",
    "    Support Vector Classifier for classification tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initiating the hyperparameters\n",
    "    def __init__(self, learning_rate = 0.001, lambda_param = 0.01, n_iterations = 1000):\n",
    "        \n",
    "        # A tuning parameter in an optimization algorithm that determines the step size at each iteration \n",
    "        self.learning_rate = learning_rate \n",
    "        # Regularization parameter to prevent overfitting\n",
    "        self.lambda_param = lambda_param\n",
    "        # Number of iterations for training\n",
    "        self.n_iterations = n_iterations\n",
    "        \n",
    "    \n",
    "    # Fitting the dataset to the SVM Classifier\n",
    "    def fit(self, X, Y):\n",
    "        \n",
    "        # n_samples = number of data points, number of rows\n",
    "        # n_features = number of input features, number of columns\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.w = np.zeros(n_features) # initalizing with 0\n",
    "        self.b = 0 # initializing with 0\n",
    "\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "        # Implementing Gradient Descent algorithm for Optimization\n",
    "        for iteration in range(self.n_iterations):\n",
    "            self.update_weights()\n",
    "    \n",
    "    \n",
    "    # Updating the weights and biases\n",
    "    def update_weights(self):\n",
    "        \n",
    "        # In SVM, the labels are expected to be -1 and 1 \n",
    "        # If it is -1 it lies on one side of the hyperplane and if it is 1 it lies on the other side of the hyperplane\n",
    "        Y_label = np.where(self.Y <= 0, -1, 1)\n",
    "        \n",
    "        for index, xi in enumerate(self.X):\n",
    "            \n",
    "            # If a point is correctly classified & outside margin ‚Üí update only by regularization term\n",
    "            # Condition: Yi * (w*Xi - b) >= 1 | correctly classified and lies outside the margin\n",
    "            if(Y_label[index] * (np.dot(xi, self.w) - self.b)) >= 1:\n",
    "                dw = 2 * self.lambda_param * self.w\n",
    "                db = 0\n",
    "\n",
    "            # If a point is inside margin/misclassified ‚Üí apply both regularization + hinge loss correction\n",
    "            # Condition: Yi * (w*Xi - b) < 1 | Point inside margin or misclassified\n",
    "            else:\n",
    "                dw = 2 * self.lambda_param * self.w - (np.dot(xi, Y_label[index])) \n",
    "                db = Y_label[index]\n",
    "\n",
    "        # Classic gradient descent update\n",
    "        self.w -= self.learning_rate * dw\n",
    "        self.b -= self.learning_rate * db\n",
    "    \n",
    "\n",
    "    # Predicting the label for a given input\n",
    "    def predict(self, X):\n",
    "        output = np.dot(X, self.w) - self.b\n",
    "\n",
    "        # Applies the sign function, matches SVM‚Äôs labeling convention (-1 and +1)\n",
    "        predicted_labels = np.sign(output)\n",
    "        \n",
    "        Y_hat = np.where(predicted_labels <= -1, 0, 1)\n",
    "\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81088a25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
